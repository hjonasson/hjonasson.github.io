---
layout: post
title: "Continuous Integration for Data Science"
categories: misc
---

My one fear of coming here was that this would be an in-person Coursera course. Although Coursera presents a lot of good material, I don't feel that it will make you a good data scientist. Gladly this was not the case. The school is as promised, about solving a real-life scenario under the guidance of experts. This includes what fundamentally lacks in most academic settings, creativity. The projects we have in our hands are mostly given to us by organisations that understand that there is some value in using their data, and are willing to invest in machine learning to see what comes out. In most cases it us up to us, the students, to demonstrate what that would be, and to implement it. Too often I have seen data science departments reduced to software development teams by decisions about data science coming from outsiders that can lack a full view of the possibilities of DS. This is a practice that can probably work just fine, but often it doesn't. In that sense they are giving us a good idea of how a good data scientist should, in my opinion, work.

That being said, data scientist can probably learn more from software developers than vice versa. Software developers have far more established ways of messing things up less, than we do. This has been a lot in the discussion lately, for example [here](https://www.locallyoptimistic.com/post/agile-analytics-p1/), where they discuss what we can and can not take from Agile development. I share most of their opinions and experiences and this is a great read. There is a thing they don't touch on which PI School does, [continuous integration](https://en.wikipedia.org/wiki/Continuous_integration), or more what Wikipedia words without naming:

>A complementary practice to CI is that before submitting work, each programmer must do a complete build and run (and pass) all unit tests. Integration tests are usually run automatically on a CI server when it detects a new commit.

Each project has an AWS instance that:
* Sets up the environment with the correct version of all packages
* Processes data
* Proves that your model will train without finishing training(something of an early stage unit test I might say)
* Takes in your model, predicts on the validation set and logs whatever metrics you have chosen to track.
* Probably other things I am forgetting

In my eyes this is a novelty. Having sometimes used other tools than Python 2.7, I have been through what I learned on this same Wikipedia page is called integration hell too many times (why doesn't everybody have the same version of Spark as I do?!?!). I have also waited for my model to train only to realise afterwards that I had scaled a column twice and done various other shenanigans I wish I would have figured out sooner. This is definitely not perfect and far from bulletproof but this practice might rid us of some of that 'waiting for the wrong thing' time we all know (right??). In this sense there are ambitious goals the school has, not just for this batch of students but for data scientists in Europe as a whole.

